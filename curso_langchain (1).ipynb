{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAAvq9aXUR7J"
      },
      "outputs": [],
      "source": [
        "# !pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NobqRIXxW8li"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuGeq1eWXLMB"
      },
      "outputs": [],
      "source": [
        "prompt = 'Qual é a capital do Brasil?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHrsDN5aXhD5"
      },
      "outputs": [],
      "source": [
        "# # Deprecated\n",
        "# openai.Completion.create(\n",
        "#       model = 'text-davinci-003', # Modelo mais voltado ao complemento de texto\n",
        "#       prompt=prompt, # prompt ==  informações passadas para a IA responder\n",
        "#       temperature=0 # Nivel de aleatoriedade nas respostas\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFVtbN4WmT8I"
      },
      "source": [
        "### Modelo para enviar um request para a API do OpenAi\n",
        "#### Versão API > 1.00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9x14_-ddTyG"
      },
      "outputs": [],
      "source": [
        "#OK!\n",
        "messages = [{\n",
        "    \"role\":\"user\",\n",
        "    \"content\": prompt\n",
        "\n",
        "             }]\n",
        "\n",
        "openai.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages= messages,\n",
        "    temperature = 0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u-lGzVJmA5m"
      },
      "source": [
        "### encapsulando a rotina de mensagem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pcd-7V3eH9i"
      },
      "outputs": [],
      "source": [
        "def enviar_mensagem(message):\n",
        "  messages = [{\"role\":\"user\", \"content\": message}]\n",
        "  response = openai.chat.completions.create(model = 'gpt-3.5-turbo', messages = messages, temperature = 0)\n",
        "  return response['choices'][0]['message']['content']\n",
        "\n",
        "enviar_mensagem(\"Quem ganhou a copa do mundo de 2002?\")\n",
        "\n",
        "# Output: \"O Brasil ganhou a Copa do Mundo de 2002.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L85MzV0tl4Ne"
      },
      "source": [
        "### Usando format strings no prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeTbiWvDm-Jz"
      },
      "outputs": [],
      "source": [
        "text = \"O Brasil goleia Guiné por 4 a 1 no amistoso em Barcelona, na Espanha\"\n",
        "\n",
        "lang = 'Inglês'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzb3AY1OlYrn"
      },
      "outputs": [],
      "source": [
        "prompt = f''' Traduza para o {lang} o seguinte texto:\n",
        "\n",
        "{text}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7zqesg3nLxg"
      },
      "outputs": [],
      "source": [
        "print(prompt)\n",
        "\n",
        "#Output: 'Traduza para o Inglês o seguinte texto: \\n \\n O Brasil goleia Guiné por 4 a 1 no amistoso em Barcelona, na Espanha'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFKaFtwTnbxh"
      },
      "outputs": [],
      "source": [
        "enviar_mensagem(prompt)\n",
        "\n",
        "#Output: 'Brazil beats Guinea 4-1 in friendly match in Barcelona, Spain.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBVuuKrdoCxx"
      },
      "source": [
        "### Gerando tradução por palavra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1qRuxwJnpbB"
      },
      "outputs": [],
      "source": [
        "prompt_engineered = f''' Escreva o resultado e uma lista, com a palavra original e sua tradução:\n",
        "\n",
        "{prompt}\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdB1VEMPojSz"
      },
      "outputs": [],
      "source": [
        "resultado = enviar_mensagem(prompt_engineered)\n",
        "print(resultado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGtpM5Sqpc5v"
      },
      "source": [
        "# Módulo 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnJtfh5qpkm2"
      },
      "source": [
        "## Utilizando LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iExVNbvwrm3r"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frmNm1ggpg1g"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc5mIsunqFfA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-proj-mKUSgS3Mo7wxxfVA62mdT3BlbkFJiiAvNRmlSctfBA8ive9w'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRQZTn9pqmty"
      },
      "outputs": [],
      "source": [
        "# Para o modelo do OpenAi é passado apenas o prompt\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature = 0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRgSxKpdrKNx"
      },
      "outputs": [],
      "source": [
        "# Para o modelo do ChatOpenAi é passado uma lista de mensagens\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature= 0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD8vKkLVtZK_"
      },
      "outputs": [],
      "source": [
        "prompt = ''' Traduza para o {lang} o seguinte texto:\n",
        "\n",
        "{text}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvD6QkaVszo9"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_template(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWe_75-MtUL2"
      },
      "outputs": [],
      "source": [
        "chat_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ4f8jT-ushT"
      },
      "outputs": [],
      "source": [
        "chat_template.messages[0].prompt.template\n",
        "\n",
        "#Output: ['lang', 'text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRcx7K4Yuzya"
      },
      "outputs": [],
      "source": [
        "#modelo de ZERO SHOT TEMPLATE\n",
        "prompt_engineered = chat_template.format_messages(\n",
        "    lang='francês',\n",
        "    text = '''Leclerc se chateia com Ferrari: \"Dificultando nossa própria vida\" \\n\n",
        "    Piloto de Mônaco sugeriu que equipe colocasse pneus secos para o Q2 na classificação do GP do Canadá, \\n\n",
        "    mas escuderia negou; do carro 16 acabou eliminado e obteve 11º tempo da sessão''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLuJyd3bwyFu"
      },
      "outputs": [],
      "source": [
        "response = llm(prompt_engineered)\n",
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJN73TjHx0Lq"
      },
      "outputs": [],
      "source": [
        "# modelo de FEW SHOT TAMPLATE\n",
        "\n",
        "exemplos = [\n",
        "  {\n",
        "    \"question\": \"Quem viveu mais, Muhammad Ali ou Alan Turing?\",\n",
        "    \"answer\": \"\"\"São necessárias perguntas de acompanhamento aqui? Sim.\n",
        "    Acompanhamento: Qual era a idade de Muhammad Ali quando ele morreu?\n",
        "    resposta: intermediária: Muhhamad Ali tinha 74 anos quando morreu.\n",
        "    acompanhamento: Qual era a idade de Alan Turing quando ele morreu?\n",
        "    Resposta intermediária: Alan Turing tinha 41 anos de idade quando morreu.\n",
        "    Portanto, a resposta final é: Muhammad Ali\"\"\"\n",
        "},\n",
        "{\n",
        "    \"question\": \"Quando nasceu o fundador do craigslist?\",\n",
        "    \"answer\": \"\"\"São necessárias perguntas de acompanhamento aqui? Sim.\n",
        "    Acompanhamento: Quem foi o fundador da craigslist?\n",
        "    resposta: intermediária: A craigslist foi fundada por Craig Newmark.\n",
        "    acompanhamento: Quando Craig Newmark nasceu??\n",
        "    Resposta intermediária: Craig Newmark nasceu em 6 de dezembro de 1952.\n",
        "    Portanto, a resposta final é: 6 de dezembro de 1952\"\"\"\n",
        "\n",
        "}]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irhEK4HI0Et8"
      },
      "outputs": [],
      "source": [
        "exemplo_prompt = PromptTemplate(input_variables= [\"question\", \"answer\"], template= \"Pergunta: {question}\\n{answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQPcJQyx0Zxl"
      },
      "outputs": [],
      "source": [
        "prompt = FewShotPromptTemplate(\n",
        "    examples=exemplos,\n",
        "    example_prompt = example_prompt,\n",
        "    suffix = \"Pergunta: {input}\",\n",
        "    input_variables = [\"input\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OCQCsf81cDq"
      },
      "outputs": [],
      "source": [
        "print(prompt.format(input= \"Quem ganhou mais prêmios Emmy, Mad Men ou Sopranos\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bj7E2-91r_h"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(prompt.format(input= \"Quem ganhou mais prêmios Emmy, Mad Men ou Sopranos?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04WFLwtT2E2T"
      },
      "outputs": [],
      "source": [
        "resultado = llm(prompt.format_messages())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuqN8WYU4Znc"
      },
      "source": [
        "### Seletores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pulFz_fE2LPi"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC9cuIJR4574"
      },
      "outputs": [],
      "source": [
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples = exemplos,\n",
        "    example_prompt = exemplo_prompt,\n",
        "    max_length = 25,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKUeYAMY6E0v"
      },
      "outputs": [],
      "source": [
        "prompt = FewSHotPromptTemplate(\n",
        "    example_selector = example_selector,\n",
        "    example_prompt=exemplo_prompt,\n",
        "    suffix=\"Pergunta: {input}\",\n",
        "    input_variables = [\"input\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Shb20G-O6j7E"
      },
      "outputs": [],
      "source": [
        "print(prompt.format(input=\"Quem ganhou mais prêmios Emmy, Mad Men ou Sopranos?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzosGvKf7ABK"
      },
      "source": [
        "# Parsers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC3Jcei28iPc"
      },
      "source": [
        "### Serve para formatar para algum formato, por exemplo JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OQSXA5H6-v8"
      },
      "outputs": [],
      "source": [
        "# modelo de output\n",
        "desire_output = {\n",
        "    \"is_offensive\": False,\n",
        "    \"main_point\": \"Harry is a great youtuber\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag1-Bwi7-Dx1"
      },
      "outputs": [],
      "source": [
        "# prompt (instruções de formatação)\n",
        "template_with_output_format = \"\"\" Para o seguinte comentário de um vídeo do Youtube, extraia a seguintes informações\n",
        "is_offensive: boolean - Se o comentário apresenta conteúdo ofensivo\n",
        "main_point: String - O ponto principal do comentario, em cinco palavras ou menos.kwargs=\n",
        "\n",
        "Escreva a resposta em formato JSON com as seguintes propriedades: is_offensive, main_point\n",
        "Comentário: {comment}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14DfmeTP-5vs"
      },
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_template(template_with_output_format)\n",
        "\n",
        "prompt_engineered = chat_template.format_messages(comment = comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrS23buI_0l1"
      },
      "outputs": [],
      "source": [
        "print(request.content)\n",
        "\n",
        "\n",
        "#Output:\n",
        "# {\n",
        "#     \"is_offensive\": false,\n",
        "#     \"main_point\": \"Harry cares for quality\"\n",
        "# }\n",
        "\n",
        "# Output tipo String"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr-hREnjBrBP"
      },
      "source": [
        "### Transformando o tipo de saída para dict\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-dbRPdjCotJ"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eLR3ZIWC_pJ"
      },
      "outputs": [],
      "source": [
        "# Definindo o formato de resposta\n",
        "offensive = ResponseSchema(name=\"offensive\", description=\" O comentário apresenta conteúdo ofensivo?.\")\n",
        "main_point = ResponseSchema(name=\"main_point\", description=\" Argumento central do comentário\")\n",
        "\n",
        "response_schemas = [offensive, main_point]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdKPETo3DvsS"
      },
      "outputs": [],
      "source": [
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2FhkueeEMha"
      },
      "outputs": [],
      "source": [
        "format_instructions = output_parser.get_format_instructions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRV7Z-jqEgnR"
      },
      "outputs": [],
      "source": [
        "print(format_instructions)\n",
        "\n",
        "\n",
        "#Output:\n",
        "'''\n",
        "json\n",
        "{\n",
        "  \"offensive\": string // o comentário apresenta conteúdo ofensivo?.,\n",
        "  \"main_point\": string // Argumento central do comentário\n",
        "}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxq-De9IFPtC"
      },
      "outputs": [],
      "source": [
        "template = \"\"\" Para o seguinte comentário de um vídeo do Youtube, extraia a seguintes informações\n",
        "is_offensive: boolean - Se o comentário apresenta conteúdo ofensivo\n",
        "main_point: String - O ponto principal do comentario, em cinco palavras ou menos.\n",
        "\n",
        "{format_instructions}\n",
        "Comentário: {comment}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2rXDwSfFgmK"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(template = template)\n",
        "\n",
        "messages = prompt.format_messages(comment = comment, format_instructions = format_instructions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtFHUzOoHcQE"
      },
      "outputs": [],
      "source": [
        "response = llm(messages)\n",
        "\n",
        "print(response.content)\n",
        "\n",
        "#Output:\n",
        "'''\n",
        "json\n",
        "{\n",
        "  \"offensive\": string // o comentário apresenta conteúdo ofensivo?.,\n",
        "  \"main_point\": string // Argumento central do comentário\n",
        "}\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8U7OApiIL-z"
      },
      "source": [
        "### Transformando em Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdt3Mt6IHrL6"
      },
      "outputs": [],
      "source": [
        "output_dict = output_parser.parse(response.content)\n",
        "\n",
        "\n",
        "#Output:\n",
        "\n",
        "# {\n",
        "#   \"offensive\": string // o comentário apresenta conteúdo ofensivo?.,\n",
        "#   \"main_point\": string // Argumento central do comentário\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYqAJJQZHke7"
      },
      "outputs": [],
      "source": [
        "output_dict.get('main_point')\n",
        "\n",
        "#Output: 'Harry cares for quality content'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxy2hk8JB8h"
      },
      "source": [
        "# Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNX33Z2XJSrI"
      },
      "source": [
        "### Chains (correntes) são encadeamentos de diversas chamadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8aahDDbJZ5R"
      },
      "source": [
        "##### Ex: quando se pergunta para o chat-GPT onde se fica a estátua da liberdade, e em seguida pergunta-se quantos metros de altura ela tem.\n",
        "\n",
        "##### O chat GPT vai reconhecer que você está perguntando a altura sobre o último assunto perguntado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bGBWzDwJDvM"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLNs9YfdLOw2"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Crie uma lista com as atividades necessárias para produzir o seguinte produto: {product}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIWCpUsZP0t5"
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm = llm, prompt = prompt)\n",
        "result = chain.run(product=\"Guitarra\")\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5bYgRU9PiCS"
      },
      "source": [
        "#### Aplicando uma lista de produtos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXa8bVFAPRLC"
      },
      "outputs": [],
      "source": [
        "products = [{\"product\": \"Frigideira\"}, {\"product\": \"Cafeteira\"}]\n",
        "\n",
        "\n",
        "result = chain.apply(products)\n",
        "\n",
        "\n",
        "# Frigideira\n",
        "print(result[0]['text'])\n",
        "\n",
        "# Cafeteira\n",
        "print(result[1]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moK-qIyNSUbD"
      },
      "source": [
        "#### usando SimpleSequencialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPoljt1uSZPb"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj67MJOMSjYi"
      },
      "outputs": [],
      "source": [
        "# Primeiro prompt\n",
        "first_prompt = ChatPromptTemplate.from_template(\"Crie uma lista com as atividades necessárias para produzir o seguinte produto: {product}?\")\n",
        "\n",
        "\n",
        "# Chain 1\n",
        "\n",
        "list_activities = LLMChain(llm=llm, prompt=first_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi8Vr2QjS9-y"
      },
      "outputs": [],
      "source": [
        "# segundo prompt\n",
        "\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Dentre as seguintes atividades escreva qual delas é a mais importante e o motivo: {list}\"\n",
        ")\n",
        "\n",
        "# Chain 2\n",
        "\n",
        "most_important_activity = SimpleSequentialChain(chains=[list_activities, most_important_activity], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8-c0ur5Tume"
      },
      "outputs": [],
      "source": [
        "# Combinando as Chains\n",
        "\n",
        "\n",
        "combined_chain = SimpleSequentialChain(chains=[list_activities, most_important_activity], verbose=True)\n",
        "\n",
        "\n",
        "result = combined_chain.run('Bootcamp de programação')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C0GTyWEV7cB"
      },
      "source": [
        "### Sequential Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AImQOoaPV-OP"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGWxMK1RWRW3"
      },
      "outputs": [],
      "source": [
        "# Determine o assunto da matéria\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Escreva o nome da prncipal área do conhecimento que a seguinte matéria aborda: \\n\\n{news}\"\n",
        ")\n",
        "\n",
        "\n",
        "# Chain 1 : Entrada= Matéria de jornal; Saída= Lista de tópicos\n",
        "\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key='topic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJiRmIYFXfxx"
      },
      "outputs": [],
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Cite o nome de um escritor famoso que escreve sobre a seguinte área do conhecimento:\\n\\n{topic}\"\n",
        ")\n",
        "\n",
        "#chain 2 : Entrada = Lista de tópicos, Saída = Lista de pesquisadores\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key= \"writer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Escreva um resumo da matéria\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Escreva um resumo da matéria a seguir: \\n\\n{news}\"\n",
        ")\n",
        "\n",
        "#chain 3 : Entrada = Matéria de jornal, Saída = resumo\n",
        "chain_three= LLMChain(llm=llm, prompt=third_prompt, output_key= \"summary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Escreva o resumo da matéria no estio do escritor escolhido\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Reescreva o seguinte texto no estilo de {writer}: \\n\\n {summary}\"\n",
        ")\n",
        "\n",
        "#chain 3 : Entrada = Escritor, Saída = resumo escrito por autor\n",
        "chain_four= LLMChain(llm=llm, prompt=fourth_prompt, output_key= \"writer_summary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sequenciando as chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "writer_news_chain = SequentialChain(\n",
        "    chains = [chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables[\"news\"],\n",
        "    output_variables=[\"topic\", \"writer\", \"summary\", \"writer_summary\"],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = writer_news_chain('''\n",
        "O resultado da Mega-Sena 2602 com prêmio de R$ 51.774.681,61 milhões foi divulgado neste sábado (17), em São Paulo, e uma aposta de Guarapari (ES) faturou sozinha a bolada.\n",
        "\n",
        "O novo milionário(a) fez a aposta online pelo Internet Banking da Caixa e fez uma aposta simples, gastando R$ 5,00.\n",
        "\n",
        "Os números sorteados hoje foram: 11 - 14 - 16 - 30 - 32 - 46\n",
        "\n",
        "LEIA TAMBÉM:\n",
        "- Quina de São João sorteia R$ 200 milhões: veja últimos resultados e mais curiosidades\n",
        "\n",
        "111 apostas chegaram bem perto e acertaram cinco dezenas. Para cada uma delas a Caixa vai pagar R$ 44.054,71. Os 6.285 acertadores de quatro dezenas vão receber R$ 1.111,50 cada.\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result[\"writer_summary\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Router Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Funciona como uma espécie de if, else"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Row Prompting\n",
        "##### Se dá o nome Row Prompting quando se passa para a IA informações sobre quem ele é, funciona como fosse uma hipnose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chef_template = '''Você é um chef de cozinha renomado e com muita experiência, além de ser extremamente didático.\n",
        "Se você não sabe a resposta de algo, você responde que não sabe.\n",
        "Responda a seguinte questão: {input}'''\n",
        "\n",
        "mechanic_template = '''Você é um mecânico com experiência em todos os tipos de automóveis.\n",
        "Se você não sabe a resposta de algo, você responde que não sabe.\n",
        "Responda a seguinte questão: {input}'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"gastronomy\",\n",
        "        \"description\": \"Recomendado para responder questões sobre gastronomia\",\n",
        "        \"prompt_template\": chef_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"auto mechanic\",\n",
        "        \"description\": \"Recomendado para responder questões sobre mecânica de automóveis\",\n",
        "        \"prompt_template\": mechanic_template\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts.router import MultiPromptChain\n",
        "from langchain.prompts.router.llm_router import LLMRouterChain, RouterOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "destination_chains = {}\n",
        "\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p[\"name\"]}: {p[\"description\"]}\" for p in prompt_infos]\n",
        "\n",
        "destinations_str = \"\\n\".join(destinations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_router_template = \"\"\"Dado um texto selecione o prompt mais adequado. \\\n",
        "Você receberá os nomes dos prompts disponíveis e uma descrição do motivo pelo qual o prompt é mais adequado. \\\n",
        "Você também pode revisar o texto original se achar que a revisão levará a uma resposta melhor do modelo de linguagem.\n",
        "\n",
        "<< FORMATAÇÃO >>\n",
        "Retorne um trecho de código markdown com um objeto JSON formatado da seguinte maneira:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ nome do prompt a ser usado ou \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ uma versão modificada da entrada original\n",
        "}}}}\n",
        "```\n",
        "\n",
        "LEMBRE-SE: \"destination\" DEVE ser um dos nomes de prompt candidatos  especificados abaixo OU pode ser \"DEFAULT\" se a entrada não for adequada a nenhum dos prompts candidatos.\n",
        "LEMBRE-SE: \"next_inputs\" pode ser apenas a entrada original se você achar que nenhuma modificação é necessária.\n",
        "\n",
        "<< PROMPTS CANDIDATOS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (lembre-se de incluir o ```json)>>\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "router_template = prompt_router_template.format(destinations=destinations_str)\n",
        "\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser= RouterOutputParser(),\n",
        ")\n",
        "\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chain = MultiPromptChain(router_chain = router_chain,\n",
        "                         destination_chains = destination_chains,\n",
        "                         default_chain = default_chain, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#chef_template\n",
        "result = chain.run(\"Qual é a receita de uma massa carbonara?\")\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# auto mechanic\n",
        "result = chain.run(\"como posso trocar o pneu de um carro?\")\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Chain é o nome que se dá quando existe um tratamento/processamento local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import TransformChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processamento local\n",
        "def get_first_and_last_paragraph(inputs: dict) -> dict:\n",
        "    text = inputs[\"text\"]\n",
        "    paragraphs = list(filter(None, text.split(\"\\n\\n\")))\n",
        "    shortened_text = paragraphs[0]\n",
        "    shortened_text += paragraphs[-1]\n",
        "    shortened_text = ''.join(shortened_text)\n",
        "    return {\"output_text\": shortened_text}\n",
        "\n",
        "\n",
        "transform_chain = TransformChain(\n",
        "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=get_first_and_last_paragraph\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"\"\"Escreva um resumo do seguinte texto:\n",
        "\n",
        "{output_text}\n",
        "\n",
        "Resumo:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
        "\n",
        "llm_chain = LLMChain(llm = llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequential_chain.run('''\n",
        "Jamiroquai é uma banda britânica de funk e acid jazz formada em 1992, liderada pelo cantor Jay Kay. A banda é popular no mundo todo e é um dos membros mais conhecidos do cenário acid jazz londrino do início dos anos 90, junto a outros grupos como Incognito, Galliano. Seu som é inspirado na música negra da década de 1970, e suas letras e conceitos visuais ocasionalmente lidam com o idealismo social e ambiental. Eles também se baseavam no rock, na música eletrônica e na música latina, e no palco se apresentavam com vários músicos da banda tocando ao vivo. Ao longo dos anos, Kay permaneceu consistentemente como líder em várias mudanças na formação.\n",
        "\n",
        "\n",
        "O grupo vendeu mais de 35 milhões de álbuns no mundo inteiro e ganhou um Grammy em 1997.[1] Apesar de se ter estreado como uma banda de acid jazz, o grupo explorou outros géneros musicais ao longo dos anos.\n",
        "\n",
        "O nome Jamiroquai deriva do nome da tribo nativa norte-americana Iroquoi, com a qual Jay Kay diz se identificar filosoficamente.[2]\n",
        "\n",
        "Formação da banda\n",
        "Jason \"Jay\" Kay começou a compor músicas para enviar a gravadoras. Entre elas, estava When You Gonna Learn?, escrita com 16 anos de idade.[3] De início, gravou-a em estúdio na Round House, em Camden.[4] Os produtores desta sessão alteraram-na e produziram-na com base nas tendências principais. Jay não gostou dos resultados e restaurou-os para a sua preferência, depois de uma disputa.[4] Em 1991, afiliou-se à Acid Jazz Records, depois de enviar uma gravação a cantar uma música dos Brand New Heavies.[5][6] De seguida, foi juntando membros para a banda, incluindo o seu amigo Wallis Buchanan, que tocava o didgeridoo.[4] Foi-lhe sugerido pelo seu agente que juntasse o teclista Toby Smith, mas não ficou convencido de início. Smith voltaria a encontrar a banda depois de um ato de suporte para os Brand New Heavies. Convenceu Kay a integrá-lo no grupo como teclista e co-compositor. A primeira música escrita pelos dois foi Too Young to Die.[4]\n",
        "\n",
        "Sendo Jay Kay o líder e imagem da banda, muitos referem-se a ele como Jamiroquai. Além disso, é o único signatário do contrato com a discográfica sob o nome Jamiroquai.[7] Há boatos de que a banda é o resultado de uma audição falhada de Jay para vocalista dos Brand New Heavies. Contudo, estes rumores são desmentidos por eles.[8]\n",
        "\n",
        "\n",
        "Primeiro single\n",
        "O primeiro single da banda When You Gonna Learn? originou uma disputa entre gravadoras, sendo a Sony Soho Square vencedora, com quem assinaram um contrato de oito álbuns, uma proeza para um jovem que andava de skate, tinha uma paixão por roupas vintage e usava um chapéu esquisito.\n",
        "\n",
        "Evolução da banda\n",
        "O primeiro disco, Emergency On Planet Earth, de 1993, foi o álbum que mais cópias vendeu em menos tempo, desde os tempos de Faith, de George Michael.\n",
        "\n",
        "The Return Of the Space Cowboy, de 1994, com os seus comentários satíricos e mordazes, é bastante agressivo, mas o sucesso da banda não ultrapassou as fronteiras britânicas. Entretanto, o álbum que deu a conhecer Jamiroquai a nível mundial, Travelling Without Moving, veio com um aviso preocupante, profético, sobre os perigos associados à engenharia biogenética. Tal álbum foi precedido pelo single Cosmic Girl, single de 4 faixas com remixes do êxito do grupo. O single vencedor de um Grammy e quatro prémios MTV Virtual Insanity, foi lançado no dia em que a ovelha Dolly nasceu. Este álbum conquistou a crítica e o público, tendo atingido o estatuto de platina em todo o mundo.\n",
        "\n",
        "Logo após Virtual Insanity, surgiram desavenças durante a criação do álbum Synkronized. Com o álbum quase pronto, Stuart Zender deixou a banda, que, por motivos de direitos de autor, teve de refazer o álbum, de maneira a que Zender não constasse como co-autor. Boatos dizem que Stuart Zender se queixava de que Jay ganhava mais dinheiro do que os outros membros, e estava insatisfeito com isso. Há também uma especulação de que a última faixa do Synkronized, King for a Day, reflete a visão de Kay a respeito de Zender e sua ganância.\n",
        "\n",
        "Depois de Synkronized, já sem a colaboração de Stuart Zender, o grupo colaborou em No Boundaries, um projeto que contou com vários artistas, como Pearl Jam e Alanis Morissette e cujas receitas reverteram para os refugiados do Kosovo.\n",
        "\n",
        "Em 2001, lançam A Funk Odyssey, que trouxe o hit Love Foolosophy, seguido de quatro anos a preparar Dynamite.\n",
        "\n",
        "Dynamite foi escrito e gravado em Espanha, Itália, Costa Rica, Escócia, Nova Iorque, Los Angeles e também no próprio estúdio de Jay, criado para o efeito na sua mansão em Buckinghamshire. Vale lembrar que apenas Jay Kay, Derrick McKenzie e Sola Akingbola constam da formação original do álbum.\n",
        "\n",
        "No dia 16 de agosto de 2010, foi anunciado no site oficial da banda um novo álbum intitulado Rock Dust Light Star, previsto para ser lançado em novembro de 2010.\n",
        "\n",
        "Em janeiro de 2017, a banda lança um vídeo promovendo o seu oitavo álbum, intitulado Automaton, que apresentou um primeiro single bem mais voltado para a música eletrónica; porém, outras canções lembraram os seus últimos trabalhos. Anunciaram também dez concertos na Ásia e Europa nesse ano. Começando por Tóquio, em maio, e terminando no Festival Sudoeste (Portugal), em agosto.\n",
        "\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chain APP - Chat Bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### A memória é run time, ou seja, se o servidor for desligado vai se perder o histórico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Olá! Como vai você?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# checar o que está na memória\n",
        "memory.buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checar o histórico\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tipos de memória\n",
        "\n",
        "##### ConversationBufferMemory - Salva msgs\n",
        "##### ConversationBufferWindowMemory - Salva últimas N msgs\n",
        "##### ConversationTokenBufferMemory - Salva últimos N tokens\n",
        "##### ConversationSummaryMemory - Cria um resumo da conversa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chain APP - Q&A (Question & Answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Incluir vários documentos pode ser um problema pela quantidade de tokens suportados\n",
        "##### Uma forma de resolver isso é pela técnica de Vector Store\n",
        "###### Vector Store é a técnica de divisão dos documentos em \"blocos\" representados de forma matemática"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = embeddings.embed_query('Olá! Tudo bem?')\n",
        "\n",
        "result[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install wikipedia\n",
        "# !pip install langchain[docarray]\n",
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WikipediaLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = WikipediaLoader(query=\"Seinfold\", load_max_docs=10, lang='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(map(lambda x : x.metadata['title'], docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import DocArrayInMemorySearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db = DocArrayInMemorySearch.from_documents(\n",
        "    docs,\n",
        "    embeddings\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db.similarity_search_with_score('Quem é Larry David?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_seinfeld = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={'k':1}),\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = qa_seinfeld.run('Quem é Larry David?')\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.evaluation.qa import QAGenerateChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_chain = QAGenerateChain.from_llm(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_examples = generate_chain.apply_and_parse(\n",
        "    [{\"doc\": t} for t in docs[:5]]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_examples[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = qa_seinfeld.apply(new_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.evaluation.qa import QAEvalchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chain Types\n",
        "\n",
        "#### Stuff - Agrega documentos relacionados no mesmo prompt\n",
        "#### Map Reduce - Executa paralelamente uma chain para cada documento relacionado e usa os resultados como input para produzir a reposta final\n",
        "#### Map Rerank - Executa paralelamente uma chain para cada documento relacionado e além de solicitar uma resposta, solicita um score e seleciona a reposta com score mais alto\n",
        "#### Refine - Executa sequencialmente uma chain para cada documento relacionado e vai construindo a resposta final a cada iteração\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain: Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.agents import load_tools, initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tools = load_tools([\"llm-math\", \"wikipedia\"], llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent= AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent(\"Quanto é 340 elevado na potência 0.23?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Usando o interpretador Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = create_python_agent(\n",
        "    llm,\n",
        "    tool = PythonREPLTool(),\n",
        "    verbose = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "random_array = np.random.randn(5)\n",
        "print(random_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = agent(f\"Ordene a seguinte lista de números: {random_array}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criando um agente próprio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import tool\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# O langchain entende o que é pra fazer com a função através do comentário (documentação) da função\n",
        "\n",
        "@tool # @tool para o langchain entender que é um agente\n",
        "def cep(cep: str) -> str:\n",
        "    \"\"\"Retorna o endereço completo de um CEP em formato JSON, use-a para qualquer \\\n",
        "    pergunta relacionada à pesquisa de endereços. \\\n",
        "    A entrada deve ser sempre um código postal, \\\n",
        "    e essa função sempre retornará o endereço completo em formato JSON.\"\"\"\n",
        "    return requests.get(f'https://viacep.com.br/ws/{cep}/json/').json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = initialize_agent(\n",
        "    load_tools([], llm=llm) + [cep],\n",
        "    llm,\n",
        "    agent = AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent(\"qual é o endereço completo do CEP 95020-183?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
